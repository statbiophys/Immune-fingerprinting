{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give pointers and examples on how we prepared the data and computed S & I from large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gmpy2\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import scipy.special as sp\n",
    "from itertools import count, product\n",
    "import importlib\n",
    "import mpmath\n",
    "from multiprocessing import Pool\n",
    "import matplotlib as mpl\n",
    "from glob import glob\n",
    "import olga.load_model as load_model\n",
    "import olga.generation_probability as pgen\n",
    "\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "# useful functions\n",
    "\n",
    "# really basic way of parallelizing any operation on a dataframe\n",
    "def parallelize_dataframe(df, func, args=None, n_cores=4):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    - df: dataframe to modify\n",
    "    - func: function that accept a dataframe as its first argument and return a dataframe\n",
    "    - args: additional arguments of fun\n",
    "    - n_core: number of cores to use\n",
    "    \"\"\"\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    if args is None:\n",
    "        all_args = [(d,) for d in df_split]\n",
    "    else:\n",
    "        all_args = [(d, *ags) for d, ags \n",
    "                    in zip(df_split, [args]*n_cores)]\n",
    "    pool = Pool(n_cores)\n",
    "    df = pd.concat(pool.starmap(func, all_args))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "def compute_pgen_nt(df, pgen_model):\n",
    "    df[\"pgen_nt\"] = df.ntCDR3.apply(pgen_model.compute_nt_CDR3_pgen)\n",
    "    return df\n",
    "\n",
    "def load_olga_model(model_directory=\"olga3/olga/default_models/human_T_beta/\"):\n",
    "    \"\"\" Fully load an olga model \"\"\"\n",
    "    params_file_name = f'{model_directory}model_params.txt'\n",
    "    marginals_file_name = f'{model_directory}model_marginals.txt'\n",
    "    V_anchor_pos_file = f'{model_directory}V_gene_CDR3_anchors.csv'\n",
    "    J_anchor_pos_file = f'{model_directory}J_gene_CDR3_anchors.csv'\n",
    "\n",
    "    genomic_data = load_model.GenomicDataVDJ()\n",
    "    genomic_data.load_igor_genomic_data(params_file_name, V_anchor_pos_file, J_anchor_pos_file)\n",
    "\n",
    "    generative_model = load_model.GenerativeModelVDJ()\n",
    "    generative_model.load_and_process_igor_model(marginals_file_name)\n",
    "\n",
    "    pgen_model = pgen.GenerationProbabilityVDJ(generative_model, genomic_data)\n",
    "\n",
    "    return genomic_data, generative_model, pgen_model\n",
    "\n",
    "\n",
    "def load_olga_model_VJ(model_directory=\"olga3/olga/default_models/human_T_beta/\"):\n",
    "    \"\"\" Fully load an olga model \"\"\"\n",
    "    params_file_name = f'{model_directory}model_params.txt'\n",
    "    marginals_file_name = f'{model_directory}model_marginals.txt'\n",
    "    V_anchor_pos_file = f'{model_directory}V_gene_CDR3_anchors.csv'\n",
    "    J_anchor_pos_file = f'{model_directory}J_gene_CDR3_anchors.csv'\n",
    "\n",
    "    genomic_data = load_model.GenomicDataVJ()\n",
    "    genomic_data.load_igor_genomic_data(params_file_name, V_anchor_pos_file, J_anchor_pos_file)\n",
    "\n",
    "    generative_model = load_model.GenerativeModelVJ()\n",
    "    generative_model.load_and_process_igor_model(marginals_file_name)\n",
    "\n",
    "    pgen_model = pgen.GenerationProbabilityVJ(generative_model, genomic_data)\n",
    "\n",
    "    return genomic_data, generative_model, pgen_model\n",
    "\n",
    "def find_nt_seq(long_nt, aa):\n",
    "    \"\"\"Extract from the long_nt nucleotide sequence the first part that match the amino-acid sequence\"\"\"\n",
    "    for ii in range(3):\n",
    "        seq = nt_to_aa(long_nt[ii:])\n",
    "        res = seq.find(aa)\n",
    "        if res != -1:\n",
    "            return long_nt[res*3 + ii]\n",
    "\n",
    "def estimate_S_unbiased(df1, df2=None, M=4000):\n",
    "    \"\"\"\n",
    "    Compute an estimate of S\n",
    "    @Arguments\n",
    "    - df1 & df2 are two dataframes that contains a column \"count\" (number of reads associated with each clonotype)\n",
    "        & a column \"ntCDR3\" (with no doublons)\n",
    "    - If df2 is None, compute the estimated sharing between \"numerical\" replicates\n",
    "    - M is the size of the two samples (M = M_1 = M_2)\n",
    "    \"\"\"\n",
    "    \n",
    "    if df2 is None:\n",
    "        Q = int(df1[\"count\"].sum())\n",
    "        def h(x):\n",
    "            return (1 - 2*gmpy2.comb(Q-M, x)/gmpy2.comb(Q, x) \n",
    "                    + gmpy2.comb(Q-2*M, x)/gmpy2.comb(Q, x))\n",
    "        # apply h on the column \"count\"\n",
    "        return float(np.sum(df1[\"count\"].apply(lambda x: h(int(x)))))\n",
    "    else:\n",
    "        Q1 = int(df1[\"count\"].sum())\n",
    "        Q2 = int(df2[\"count\"].sum())\n",
    "        def h(x1, x2):\n",
    "            return ((1 - gmpy2.comb(Q1-M, x1)/gmpy2.comb(Q1, x1)) \n",
    "                    * (1 - gmpy2.comb(Q2-M, x2)/gmpy2.comb(Q2, x2)))\n",
    "        # only keep shared sequences\n",
    "        df = df1.merge(df2, how='inner', on=\"ntCDR3\")\n",
    "        return float(np.sum(\n",
    "            df.apply(lambda x: h(int(x[\"count_x\"]), int(x[\"count_y\"])), \n",
    "                     axis=1)))\n",
    "    \n",
    "def estimate_I_unbiased(df1, df2=None, M=4000, gamma=12):\n",
    "    \"\"\"\n",
    "    Compute an estimate of I\n",
    "    @Arguments\n",
    "    - df1 & df2 are two dataframes that contains a column \"count\" (number of reads associated with each clonotype)\n",
    "        a column pgen_nt & a column \"ntCDR3\" (with no doublons)\n",
    "    - If df2 is None, compute the estimated sharing between \"numerical\" replicates\n",
    "    - M is the size of the two samples (M = M_1 = M_2)\n",
    "    \"\"\"\n",
    "    \n",
    "    if df2 is None:\n",
    "        Q = int(df1[\"count\"].sum())\n",
    "        def h(x):\n",
    "            return (1 - 2*gmpy2.comb(Q-M, x)/gmpy2.comb(Q, x) \n",
    "                    + gmpy2.comb(Q-2*M, x)/gmpy2.comb(Q, x))\n",
    "        # apply h on the column \"count\"\n",
    "        return float(np.sum(df1[\"count\"].apply(lambda x: h(int(x))) * df1[\"pgen_nt\"]))\n",
    "    else:\n",
    "        Q1 = int(df1[\"count\"].sum())\n",
    "        Q2 = int(df2[\"count\"].sum())\n",
    "        def h(x1, x2):\n",
    "            return ((1 - gmpy2.comb(Q1-M, x1)/gmpy2.comb(Q1, x1)) \n",
    "                    * (1 - gmpy2.comb(Q2-M, x2)/gmpy2.comb(Q2, x2)))\n",
    "        # only keep shared sequences\n",
    "        df = df1.merge(df2, how='inner', on=\"ntCDR3\")\n",
    "        return float(np.sum(\n",
    "            df.apply(lambda x: h(int(x[\"count_x\"]), int(x[\"count_y\"])), \n",
    "                     axis=1)  * (-np.log(df[\"pgen_nt_x\"] - gamma))))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yellow Fever dataset\n",
    "\n",
    "Data is associated with [Pogorely et. al](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6294963/), and is extracted from the SRA, accession number `PRJNA493983`. The dataset contains RepSeq data on three pairs of twins, with multiple time points both pre and post Yellow Fever immunization. We load already pre-processed data (migec + mixcr). Data is cDNA, and the library is generated through 5'-race. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robins dataset\n",
    "\n",
    "Data is associated with [Longitudinal immunosequencing in healthy people](https://pubmed.ncbi.nlm.nih.gov/31226930/), the dataset can be downloaded from [ImmuneAccess](https://clients.adaptivebiotech.com/immuneaccess) (adaptive biotechnologies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Emerson dataset\n",
    "\n",
    "Data is associated with [Emerson et al.](https://www.nature.com/articles/ng.3822) and can be downloaded from [ImmuneAccess](https://clients.adaptivebiotech.com/immuneaccess) (adaptive biotechnologies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Due to their sizes, only one repertoire (from the Yellow Fever Dataset) is attached. The code can take some time (it needs to compute a fair amount of generation probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:  199.7092593568384\n"
     ]
    }
   ],
   "source": [
    "# compute pgen on a dataset (YFV)\n",
    "# code rely on olga (https://github.com/statbiophys/OLGA/tree/master/olga), \n",
    "# the default models folder is also present in the olga github\n",
    "_, _, pgen_model = load_olga_model(\n",
    "    model_directory=\"../datasets/default_models/human_T_beta/\")\n",
    "\n",
    "df = pd.read_csv(f\"../datasets/S1_0_F1_.txt.gz\", sep=\"\\t\", \n",
    "                 usecols=[\"Clone count\", \"Clone fraction\", \"N. Seq. CDR3\"])\n",
    "df.columns = [\"count\", \"frequency\", \"ntCDR3\"]\n",
    "df = df.dropna(subset=[\"ntCDR3\"]) # remove sequences where the CDR3 wasn't found\n",
    "df = df[df.ntCDR3.apply(lambda x: len(x)%3 == 0)] # only keep the in-frame\n",
    "# df = parallelize_dataframe(df, compute_pgen_nt, args=(pgen_model,), n_cores=4)\n",
    "\n",
    "# estimate the (autologous) mean of S (M=5000) \n",
    "S = estimate_S_unbiased(df, df, M=5000)\n",
    "print(\"S: \", S)\n",
    "\n",
    "# estimate the (autologous) mean of I (M=5000) \n",
    "# I = estimate_I_unbiased(df, df, M=5000)\n",
    "# print(\"I: \", I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
